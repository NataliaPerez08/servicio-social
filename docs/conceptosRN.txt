# Softmax function
The softmax function, also known as softargmax or normalized exponential function,
converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a 
generalization of the logistic function to multiple dimensions, and used in multinomial logistic 
regression. The softmax function is often used as the last activation function of a neural network to 
normalize the output of a network to a probability distribution over predicted output classes, based on 
Luce's choice axiom.

## Luce's choice axiom
In probability theory, Luce's choice axiom, formulated by R. Duncan Luce (1959), states that the 
probability of selecting one item over another from a pool of many items is not affected by the presence 
or absence of other items in the pool. Selection of this kind is said to have "independence from 
irrelevant alternatives" 

## Activation function
The activation function of a node in an artificial neural network is a function that calculates the 
output of the node based on its individual inputs and their weights. Nontrivial problems can be solved 
using only a few nodes if the activation function is nonlinear. Modern activation functions include 
the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model, the logistic 
(sigmoid) function used in the 2012 speech recognition model developed by Hinton et al,
the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model.